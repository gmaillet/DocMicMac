\chapter{Approches \'energ\'etiques et r\'egularisation}

    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\section{G\'en\'eralit\'es}

\label{DUMG:Appr:Energ}

Le travail d'un corr\'elateur consiste \`a calculer une fonction
\FPx , poss\'edant certaines caract\'eristiques \emph{a priori}
(de r\'egularit\'e)  et
telle que les $Im_k(\pi_k(x,y,\FPx(x,y))),k\in [1,N]$ se ressemblent.

On note :

\begin{itemize}
     \item $\Ress(x,y,p_x) \geq 0$  un crit\`ere qui mesure la
            ressemblance locale  entre les
            $Im_k(\pi_k(x,y),p_x)_{k\in [1,N]}$, avec $\Ress=0$
            quand les images se ressemblent parfaitement (crit\`ere
            d'attache aux donn\'ees);

      \item $\nabla(\FPx)$ le gradient de \FPx,
             $\nabla(U) = (\frac{U}{\partial x}, \frac{U}{\partial y})$

      \item $\| \nabla(\FPx) \|^{reg}$ une norme sur le gradient, qui
            est utilis\'e comme crit\`ere de r\'egularisation (p\'enalisation
            des variation de \FPx);
\end{itemize}

L'approche \'energ\`etique consiste \`a chercher
une solution \FPx qui minimise une fonctionnelle d'\'energie $\Energ(\FPx)$  :

\begin{equation}
   \Energ(\FPx) = \iint_\ETer \Ress(x,y,\FPx(x,y)) + \| \nabla(\FPx) \|^{reg}
\end{equation}

Pour les probl\`emes relevant de l'optimisation combinatoire
MicMac utilise souvent une norme $L_1$ :

\begin{equation}
\label{CostGrad:Lun:DDeux2}
   \| \nabla(\FPx) \|^{reg} = \alpha_1*|\nabla(\FPx^1)|+\alpha_2*|\nabla(\FPx^2)|
\end{equation}

Le(s) param\`etre(s) $\alpha$ permet de pond\'erer l'importance relative
de l'\emph{a priori} et de l'attache aux donn\'ees.
Lorsque  $\DimPx=2$, on a typiquement
$\alpha_1 \approx \alpha_2$ pour les g\'eom\'etries "vraiment"
bi-dimensionnelles et  $\alpha_1 \ll \alpha_2$ pour les
g\'eom\'etries o\`u $\FPx^2$ sert \`a mod\'eliser des d\'efauts de mise
en place.

Dans certains cas MicMac peut utiliser aussi des normes $L_2$.
En programmation dynamique on ne fait intervenir que les d\'eriv\'ees
premi\`eres. Pour les algorithmes variationnels, il est aussi
possibles d'introduire les d\'eriv\`ees secondes.

\begin{tabular} { c | c | c | c| c| c | c} % \\  \hline

 {\bf Type }        &  {Quant} & {Rapidit\'e} & {Der} &  {DPax} &  {D Opt} & {Optim}\\  \hline \hline
 {\bf Prog Dyn }    &  {Oui} & {+++} &          {[0-1]} & {1,2} & {1} &{approch\'ee}\\  \hline
 {\bf Cox-Roy }     &  {Oui} & {++} &           {[0-0]} & {1} & {1,2} & {exacte}\\  \hline
 {\bf Differentiel }&  {Non} & {+} &            {[1-2]} & {1,2} & {1,2} &{locale} \\  \hline  \hline
 {\it Dequant }     &  {?} & {++++} &           {[0-0]} & {1,2} & {1,2} &{filtrage}\\  \hline
 {\it MaxOfScore }  &  {Oui} & {++++} &         {[0-0]} & {1,2} & {1,2} &{aucune} \\  \hline

\end{tabular}



    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\section{Programmation dynamique}

La programmation dynamique se limite au cas o\`u l'espace de d\'epart est
ordonn\'e (donc de dimension $1$) et l'espace d'arriv\'ee est quantifi\'e.
Ces limitations prises en compte, c'est une
une m\'ethode de port\'ee assez g\'en\'erale
pour optimiser   des application de  $\ZZ$ dans $\ZZ^k$.

En st\'er\'eo vision (et en traitement d'image de mani\`ere plus
g\'en\'erale)  o\`u l'espace de d\'epart est de dimension $2$,
on est souvent conduit \`a balayer l'image suivant les
lignes (ou les colonnes, ou n'importe quelle direction)
pour utiliser la programmation dynamique.
Pour chaque balayage on a alors une application de $\ZZ$ dans $\ZZ^k$.


Formellement, on peut d\'ecrire ainsi le fonctionnement :

\begin{itemize}
   \item on dispose de $N$ positions $P_k ,  k \in [1,N]$;

   \item pour chaque position, on dispose de  l'ensemble des
         \'etats possibles que peut prendre cette position
         $E_k ={e^k_1,\dots,e^k_{n_k}} $

   \item une solution $S$ est une suite correspondant \`a la
          s\'election de l'\'etat de chaque position $\{e^1_{S(1}) \dots e^N_{S(N)}\}$;

    \item  chaque \'etat poss\`ede un cout intrins\`eque $C^I(e^k_i)$ et chaque
           couple d'\'etats successifs poss\`ede un co\^ut de transition $C^T(e^k_i,e^{k+1}_j)$
\end{itemize}

Le co\^ut global d'une solution est d\'efini par :

\begin{equation}
  \mathcal C (S) = \sum_{k=1}^N  C^I(e^k_{S(k)}) + C^T(e^k_{S(k)},e^{k+1}_{S(k+1)})
\end{equation}

Une fois effectu\'e l'op\'eration de balayage, la transcription pour chaque
ligne au probl\`eme d'appariement  est imm\'ediate :

\begin{itemize}
    \item les positions sont les pixels;

    \item les \'etats d'un pixels sont les paralaxes qu'il peut prendre
          en fonction du contexte (nappes englobantes \dots);

    \item une solution est un champs de paralaxe;

    \item les co\^uts intrins\`eques portent  l'attache
          aux donn\'ees (fonction de la corr\'elation obtenue pour chaque pixel
          lorsqu'on le reprojette \`a la parallaxe $e^k_j$);

    \item les co\^uts  de transitions  portent  l'a priori;
          par exemple, si la paralaxe correspondant \`a
          l'\'etat $e^k_i$  s'\'ecrit $({Px^1}^k_i,{Px^2}^k_i)$, on pourrait
          utiliser l'\'equation~\ref{CostPrgDyn:DDeux} comme sch\'ema de
          discr\'etisation de~\ref{CostGrad:Lun:DDeux2};

    \item on pourrait  aussi  mod\'eliser des co\^uts quadratiques avec
          l'\'equation~\ref{CostPrgDyn:Quad};

    \item on peut imposer diff\'erentes contraintes, par exemple
           que les solution retenue respectent un crit\`ere
          de pente maximum $ \mathcal P_{max}$  en rajoutant une r\`egle du type
          de l'\'equation~\ref{CostPrgDyn:Pmax};


\end{itemize}


\begin{equation}
\label{CostPrgDyn:DDeux}
C^T(e^k_i,e^{k+1}_j) =    \alpha_1* |{Px^1}^k_i-{Px^1}^{k+1}_j|
                          + \alpha_2* |{Px^2}^k_i-{Px^2}^{k+1}_j|
\end{equation}

\begin{equation}
\label{CostPrgDyn:Quad}
C^T(e^k_i,e^{k+1}_j) =    \alpha_1* ({Px^1}^k_i-{Px^1}^{k+1}_j)^2
\end{equation}

\begin{equation}
\label{CostPrgDyn:Pmax}
   |{Px^1}^k_i-{Px^1}^{k+1}_j| > \mathcal P_{max} \Rightarrow  C^T(e^k_i,e^{k+1}_j) = + \infty
\end{equation}


La programmation dynamique fournit alors un algorithme rapide pour
calculer la solution $\mathcal S_{min} $ qui minimise le
co\^ut $ \mathcal C (S) $.
Rappelons le principe :

\begin{itemize}

\label{Def:CPlusMin}
   \item on cherche \`a calculer la fonction $ \mathcal C^+_{min} (e^k_i)$
   qui correspondent au co\^ut de la plus petite sous suite de
   taille $k$ se  terminant par $k$ (c.a.d.  les suites
   de la forme $e^0_{i_0} \dots e^{k-1}_{i_{k-1}} e^k_i$);

   \item on effectue le calcul en parcourant les positions
         suivant les indice croissants;

   \item  en rajoutant un sommet neutre d'indice $0$ en d\'ebut on a
          initialement  $ \mathcal C^+_{min} (e^0_0) = 0$

   \item on utilise ensuite la r\'ecurence donn\'ee par
         l'\'equation~\ref{CostPrgDyn:Recur}, en memorisant
         pour chaque sommet celui ayant conduit au calcul
         du minimum ( son "p\`ere");

\end{itemize}

\begin{equation}
\label{CostPrgDyn:Recur}
 \mathcal C^+_{min} (e^{k+1}_i) =  C^I(e^{k+1}_i)+Min_{j\in[1,n_k]}(
             \mathcal  C^+_{min} (e^k_j) +  C^T(e^k_j,e^{k+1}_i)
 )
\end{equation}

Arriv\'e  \`a la position $N$, l'\'etat qui minimise $\mathcal  C^+_{min} (e^N_j) $
est celui par lequel se termine  la solution $\mathcal S_{min} $, il
suffit alors de parcourir les position vers l'arri\`ere \`a partir de cet
\'etat en remontant aux "p\`eres" pour obtenir l'ensemble de  la
s\'erie $\mathcal S_{min} $.


    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\section{Programmation dynamique "multi directionnelle"}

\label{ProgDyn:MulDir}

Le balayage directionnel, n\'ecessaire pour appliquer la
programmation dynamique \`a des images, conduit \`a des
artefacts qui peuvent \^etre assez g\'enants.
MicMac propose un principe de programmation dynamique "multi-directionnel"
qui permet de limiter ces effets.
Pour chaque direction de balayage, on
calcul e:

\begin{itemize}
   \item d'une part $ \mathcal C^+_{min} (e^k_i)$ d\'ecrit
    en~\ref{Def:CPlusMin};

   \item d'autre part $ \mathcal C^-_{min} (e^k_i)$ correspondant
   au co\^ut de la plus petite sous suite de
   taille $N-k$  commen\c{c}ant par $k$  et calcul\'ee selon
   le m\^eme principe  (mais par un parcours arri\`ere);

\end{itemize}

On pose :

\begin{equation}
  \mathcal C_{min}(e^k_i) = \mathcal C^+_{min} (e^k_i) + \mathcal C^-_{min} (e^k_i) - C^I(e^k_i)
\end{equation}

On montre facilement que $C_{min}(e^k_i)$ est le co\^ut de la plus
petite solution contrainte \`a passer par $e^k_i$.
On pose finalement :

\begin{equation}
    \Delta_{min}(e^k_i) = \mathcal C_{min}(e^k_i) -  \mathcal C ( \mathcal S_{min})
\end{equation}

La valeur $\Delta_{min}(e^k_i)$ s'interpr\`ete facilement comme
le surco\^ut, par rapport \`a la solution optimale qu'il y a \`a passer
par l'\'etat $e^k_i$. On dispose d'une information sensiblement
plus riche que la programmation dynamique  habituelle qui ne fournit
que la solution optimale : on dispose maintenant, pour chaque \'etat,
d'une mesure \`a valeur r\'eelle quantifiant  quelle distance, au sens
du crit\`ere \`a minimiser, le s\'epare de la solution optimum.


L'int\'er\^et est d'avoir une mesure qui va permettre d'aggr\'eger les
r\'esultat obtenus par des balayages effectu\'es dans plusieurs
directions de l'images afin de limiter les effets directionnels.
On poura par exemple balayer l'image selon $D$ directions
correspondant \`a des angles $\frac{d\pi}{D}$ et aggr\'eger
suivant une des m\'ethodes ci-dessous chaque $\Delta^d_{min}(e^k_i)$ obtenu~:

\begin{enumerate}
   \item calculer la moyenne des $\Delta^d_{min}(e^k_i)$;
   \item calculer le max des $\Delta^d_{min}(e^k_i)$;
   \item utiliser chaque $\Delta^d_{min}(e^k_i)$ comme co\^ut d'entr\'ee
         pour le prochain balyage (m\'ethode dite r\'eentrante);
\end{enumerate}




    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\section{Algorithmes de flots}

Lorsque la parallaxe est de dimension $1$,
les algorithmes de "flots maximum / coupe minimum" permettent
de trouver le minimum exact du cit\`ere \'energ\'etique
en $L_1$. Une fois le prob\`eme discr\'etis\'e et quantifi\'e,
la fonctionnelle s'\'ecrit :


\begin{equation}
  \label{CostGrad:MaxFlot}
  E(Z) = \Sigma _{x,y}  (A(x,y,Z(x,y)) + \alpha *  \Sigma _{u,v \in V}P^{ds}_{u,v}|Z(x,y)-Z(x+u,y+v)|)
\end{equation}

Avec $Z$ la fonction inconnue, $A$ le crit\`ere d'attache aux donn\'ees,
$\alpha$ le coefficient pond\'erant l'\emph{a priori}, $V$ le voisinage
utilis\'ee (typiquement $4$ ou $8$ voisinage), et $P^{ds}_{u,v}$ une
\'eventuelle pond\'eration du voisinage.
L'impl\'ementation propos\'ee par MicMac est bas\'ee sur le code
de Cox \& Roy d\'ecrite dans~\cite{CoxRoy}.




    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\section{Algorithmes de d\'equantification}

\label{DUMG:DEQUANTIF}

    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


\section{Algorithmes variationnels}

Pas encore implant\'es.



